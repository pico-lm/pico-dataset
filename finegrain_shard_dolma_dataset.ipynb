{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dolma Dataset Fine-Grained Sharding\n",
    "\n",
    "We use this notebook to further shard the dataset that is created by calling `create_dolma_dataset.py`.\n",
    "\n",
    "The original dataset consists of 100 shards, and this notebook resizes it to 10,000 smaller shards for more efficient processing and distribution.\n",
    "\n",
    "- **Original dataset**: 100 shards with 2,048,000 samples per shard\n",
    "- **New dataset**: 10,000 shards with 20,480 samples per shard (100Ã— more granular)\n",
    "- The notebook creates these new shards locally and uploads them to Hugging Face Hub as a new dataset: `pico-lm/pretokenized-dolma_v2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up environment variables for HuggingFace token\n",
    "with open(\".env\", \"r\") as file:\n",
    "    for line in file:\n",
    "        if line.startswith(\"HF_TOKEN\"):\n",
    "            HF_TOKEN = line.split(\"=\")[1].strip()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "original_num_shards = 100\n",
    "new_num_shards = 10000\n",
    "\n",
    "original_num_samples_per_shard = 2_048_000\n",
    "new_num_samples_per_shard = 20_480\n",
    "\n",
    "# NOTE: create a new dataset parquet files that we then need to upload to HF\n",
    "# stores data in tmp_data/new_data and uses the tmp_data/cache directory for caching\n",
    "\n",
    "os.makedirs(\"tmp_data/cache\", exist_ok=True)\n",
    "\n",
    "# Iterate over each shard of the original dataset and split it into 100 new shards\n",
    "for original_shard_idx in range(original_num_shards):\n",
    "    print(f\"Processing shard {original_shard_idx}\")\n",
    "    original_shard_idx_str = str(original_shard_idx).zfill(3)\n",
    "    ds = load_dataset(\"pico-lm/pretokenized-dolma\", split=\"train\", data_files=f\"data/train-{original_shard_idx_str}-of-100.parquet\", cache_dir=\"tmp_data/cache\", num_proc=10)\n",
    "\n",
    "    curr_shard_dir = f\"tmp_data/new_data/shard_{original_shard_idx}\"\n",
    "    os.makedirs(curr_shard_dir, exist_ok=True)\n",
    "\n",
    "    for new_shard_idx in range(new_num_shards//original_num_shards):\n",
    "\n",
    "        dataset_shard = ds.shard(num_shards=100, index=new_shard_idx)\n",
    "\n",
    "        idx_start = original_shard_idx * original_num_samples_per_shard + new_shard_idx * new_num_samples_per_shard\n",
    "        shard = dataset_shard.add_column(\"idx\", range(idx_start, idx_start + new_num_samples_per_shard))\n",
    "\n",
    "        shard_id = str(original_shard_idx * 100 + new_shard_idx).zfill(5)\n",
    "\n",
    "        shard_file_name = f\"train-{shard_id}-of-10000.parquet\"\n",
    "\n",
    "        hf_file_path = f\"data/{shard_file_name}\"\n",
    "        shard_file_path = os.path.join(curr_shard_dir, shard_file_name)\n",
    "\n",
    "        shard.to_parquet(shard_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "HF_REPO = \"pico-lm/pretokenized-dolma_v2\"\n",
    "\n",
    "api = HfApi()\n",
    "api.create_repo(HF_REPO, exist_ok=True, token=HF_TOKEN, repo_type=\"dataset\")\n",
    "\n",
    "# Upload the new shards to the hub\n",
    "\n",
    "for shard_idx in range(0, 100):\n",
    "    api.upload_folder(\n",
    "        folder_path=f\"/home/rd654/pico-dataset/tmp_data/new_data/shard_{shard_idx}\",\n",
    "        path_in_repo=\"data\",\n",
    "        repo_id=\"pico-lm/pretokenized-dolma_v2\",\n",
    "        token=HF_TOKEN,\n",
    "        repo_type=\"dataset\",\n",
    "    )\n",
    "\n",
    "    # NOTE: We originally named this new dataset `pretokenized-dolma_v2`; what we did is later we \n",
    "    # manually renamed it to `pretokenized-dolma` on Hugging Face and deleted the version created by `create_dolma_dataset.py`\n",
    "    # In other words, this is the dataset that you see on the Hugging Face Hub."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

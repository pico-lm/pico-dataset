{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paloma Dataset Creation for PICO-LM\n",
    "\n",
    "This notebook processes the Paloma dataset from Allen AI to create two tokenized versions:\n",
    "1. `pretokenized-paloma`: The complete tokenized dataset\n",
    "2. `pretokenized-paloma-tinsy`: A smaller subset containing up to 100 examples from each source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining basic constants: names of the datasets and the sequence length\n",
    "\n",
    "config_names = [\n",
    "    \"4chan_meta_sep\",\n",
    "    \"c4_100_domains\",\n",
    "    \"c4_en\",\n",
    "    \"dolma_100_programing_languages\",\n",
    "    \"dolma_100_subreddits\",\n",
    "    \"dolma-v1_5\",\n",
    "    \"falcon-refinedweb\",\n",
    "    \"gab\",\n",
    "    \"m2d2_s2orc_unsplit\",\n",
    "    \"m2d2_wikipedia_unsplit\",\n",
    "    \"manosphere_meta_sep\",\n",
    "    \"mc4\",\n",
    "    \"ptb\",\n",
    "    \"redpajama\",\n",
    "    \"twitterAAE_HELM_fixed\",\n",
    "    \"wikitext_103\"\n",
    "]\n",
    "\n",
    "SEQ_LEN = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# NOTE: this is the same tokenizer we use for the dolma preprocessing\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-7b-0724-hf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up environment variables for HuggingFace token\n",
    "with open('.env', 'r') as f:\n",
    "    for line in f:\n",
    "        if line.strip() and not line.startswith('#'):\n",
    "            key, value = line.strip().split('=', 1)\n",
    "            os.environ[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_chunk(examples):\n",
    "    \"\"\"Tokenizes and chunks text data to SEQ_LEN sequences.\"\"\"\n",
    "    tokens = []\n",
    "    source = examples['source'][0] # NOTE: only one source per dataset\n",
    "\n",
    "    for text in examples['text']:\n",
    "        _tokens = tokenizer.encode(text)\n",
    "        _tokens.append(tokenizer.eos_token_id)\n",
    "        tokens.extend(_tokens)\n",
    "\n",
    "    # Split tokens into chunks of SEQ_LEN\n",
    "    chunks = [tokens[i:i + SEQ_LEN] for i in range(0, len(tokens), SEQ_LEN)]\n",
    "    texts = [tokenizer.decode(chunk) for chunk in chunks]\n",
    "    sources = [source] * len(texts)\n",
    "\n",
    "    # print(len(chunks))\n",
    "    \n",
    "    # Discard the last chunk if it's shorter than SEQ_LEN\n",
    "    if len(chunks[-1]) < SEQ_LEN:\n",
    "        chunks = chunks[:-1]\n",
    "        texts = texts[:-1]\n",
    "        sources = sources[:-1]\n",
    "        \n",
    "    return {'input_ids': chunks, 'text': texts, 'source': sources}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store 'all' and 'subsampled' datasets \n",
    "# NOTE: all will correspond to the entire pretokenized-paloma dataset while \n",
    "# subsampled will correspond to the pretokenized-paloma-tinsy dataset\n",
    "\n",
    "all_datasets = []\n",
    "subsampled_datasets = []\n",
    "\n",
    "# NOTE: the paloma dataset is stored in multiple files, so we need to load each file separately\n",
    "for config_name in config_names:\n",
    "    sub_dataset = load_dataset(\"allenai/paloma\", config_name, split='val', token=os.environ[\"HF_TOKEN\"])\n",
    "    shuffled_sub_dataset = sub_dataset.shuffle(seed=42)  # Set seed for reproducibility\n",
    "\n",
    "    # Tokenizing and chunking the dataset\n",
    "    tokenized_sub_dataset = shuffled_sub_dataset.map(\n",
    "        tokenize_and_chunk,\n",
    "        remove_columns=shuffled_sub_dataset.column_names,\n",
    "        batched=True,\n",
    "        batch_size=100,\n",
    "        num_proc=70,\n",
    "        keep_in_memory=True,\n",
    "    )\n",
    "    \n",
    "    # Take up to 100 rows from each dataset\n",
    "    subsampled_dataset = tokenized_sub_dataset.shuffle(seed=42).select(range(min(100, len(tokenized_sub_dataset))))\n",
    "    subsampled_datasets.append(subsampled_dataset)\n",
    "\n",
    "    all_datasets.append(tokenized_sub_dataset)\n",
    "\n",
    "# Combine all datasets into one\n",
    "combined_subsampled_dataset = concatenate_datasets(subsampled_datasets)\n",
    "combined_all_datasets = concatenate_datasets(all_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pushing the datasets to the hub\n",
    "combined_subsampled_dataset.push_to_hub(\"pico-lm/pretokenized-paloma-tinsy\",  token=os.environ[\"HF_TOKEN\"], split=\"val\")\n",
    "combined_all_datasets.push_to_hub(\"pico-lm/pretokenized-paloma\",  token=os.environ[\"HF_TOKEN\"], split=\"val\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

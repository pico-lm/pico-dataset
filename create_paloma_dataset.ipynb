{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_names = [\n",
    "    \"4chan_meta_sep\",\n",
    "    \"c4_100_domains\",\n",
    "    \"c4_en\",\n",
    "    \"dolma_100_programing_languages\",\n",
    "    \"dolma_100_subreddits\",\n",
    "    \"dolma-v1_5\",\n",
    "    \"falcon-refinedweb\",\n",
    "    \"gab\",\n",
    "    \"m2d2_s2orc_unsplit\",\n",
    "    \"m2d2_wikipedia_unsplit\",\n",
    "    \"manosphere_meta_sep\",\n",
    "    \"mc4\",\n",
    "    \"ptb\",\n",
    "    \"redpajama\",\n",
    "    \"twitterAAE_HELM_fixed\",\n",
    "    \"wikitext_103\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-7b-0724-hf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the .env file and set environment variables\n",
    "with open('.env', 'r') as f:\n",
    "    for line in f:\n",
    "        if line.strip() and not line.startswith('#'):\n",
    "            key, value = line.strip().split('=', 1)\n",
    "            os.environ[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_chunk(examples):\n",
    "    \"\"\"Tokenizes and chunks text data to SEQ_LEN sequences.\"\"\"\n",
    "    tokens = []\n",
    "    source = examples['source'][0] # NOTE: only one source per dataset\n",
    "\n",
    "    for text in examples['text']:\n",
    "        _tokens = tokenizer.encode(text)\n",
    "        _tokens.append(tokenizer.eos_token_id)\n",
    "        tokens.extend(_tokens)\n",
    "\n",
    "    # Split tokens into chunks of SEQ_LEN\n",
    "    chunks = [tokens[i:i + SEQ_LEN] for i in range(0, len(tokens), SEQ_LEN)]\n",
    "    texts = [tokenizer.decode(chunk) for chunk in chunks]\n",
    "    sources = [source] * len(texts)\n",
    "\n",
    "    # print(len(chunks))\n",
    "    \n",
    "    # Discard the last chunk if it's shorter than SEQ_LEN\n",
    "    if len(chunks[-1]) < SEQ_LEN:\n",
    "        chunks = chunks[:-1]\n",
    "        texts = texts[:-1]\n",
    "        sources = sources[:-1]\n",
    "        \n",
    "    return {'input_ids': chunks, 'text': texts, 'source': sources}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since allenai/paloma couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration '4chan_meta_sep' at /home/rd654/.cache/huggingface/datasets/allenai___paloma/4chan_meta_sep/0.0.0/65cd6fc59dba021b21db414fa5e8d7765ffbe5e6 (last modified on Sun Dec  1 16:30:34 2024).\n",
      "Map (num_proc=70): 100%|██████████| 523/523 [00:02<00:00, 261.03 examples/s]\n",
      "Using the latest cached version of the dataset since allenai/paloma couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'c4_100_domains' at /home/rd654/.cache/huggingface/datasets/allenai___paloma/c4_100_domains/0.0.0/65cd6fc59dba021b21db414fa5e8d7765ffbe5e6 (last modified on Sun Dec  1 16:30:36 2024).\n",
      "Map (num_proc=70): 100%|██████████| 14059/14059 [00:14<00:00, 937.92 examples/s] \n",
      "Using the latest cached version of the dataset since allenai/paloma couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'c4_en' at /home/rd654/.cache/huggingface/datasets/allenai___paloma/c4_en/0.0.0/65cd6fc59dba021b21db414fa5e8d7765ffbe5e6 (last modified on Sun Dec  1 16:30:37 2024).\n",
      "Map (num_proc=70): 100%|██████████| 2091/2091 [00:04<00:00, 497.40 examples/s] \n",
      "Using the latest cached version of the dataset since allenai/paloma couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'dolma_100_programing_languages' at /home/rd654/.cache/huggingface/datasets/allenai___paloma/dolma_100_programing_languages/0.0.0/65cd6fc59dba021b21db414fa5e8d7765ffbe5e6 (last modified on Sun Dec  1 16:30:39 2024).\n",
      "Map (num_proc=70): 100%|██████████| 8123/8123 [00:13<00:00, 612.56 examples/s] \n",
      "Using the latest cached version of the dataset since allenai/paloma couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'dolma_100_subreddits' at /home/rd654/.cache/huggingface/datasets/allenai___paloma/dolma_100_subreddits/0.0.0/65cd6fc59dba021b21db414fa5e8d7765ffbe5e6 (last modified on Sun Dec  1 16:30:40 2024).\n",
      "Map (num_proc=70): 100%|██████████| 46347/46347 [00:14<00:00, 3090.27 examples/s]\n",
      "Using the latest cached version of the dataset since allenai/paloma couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'dolma-v1_5' at /home/rd654/.cache/huggingface/datasets/allenai___paloma/dolma-v1_5/0.0.0/65cd6fc59dba021b21db414fa5e8d7765ffbe5e6 (last modified on Sun Dec  1 16:30:42 2024).\n",
      "Map (num_proc=70): 100%|██████████| 4600/4600 [00:07<00:00, 585.07 examples/s] \n",
      "Using the latest cached version of the dataset since allenai/paloma couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'falcon-refinedweb' at /home/rd654/.cache/huggingface/datasets/allenai___paloma/falcon-refinedweb/0.0.0/65cd6fc59dba021b21db414fa5e8d7765ffbe5e6 (last modified on Sun Dec  1 16:30:43 2024).\n",
      "Map (num_proc=70): 100%|██████████| 1553/1553 [00:02<00:00, 712.20 examples/s] \n",
      "Using the latest cached version of the dataset since allenai/paloma couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'gab' at /home/rd654/.cache/huggingface/datasets/allenai___paloma/gab/0.0.0/65cd6fc59dba021b21db414fa5e8d7765ffbe5e6 (last modified on Sun Dec  1 16:30:44 2024).\n",
      "Map (num_proc=70): 100%|██████████| 23249/23249 [00:02<00:00, 10673.47 examples/s]\n",
      "Using the latest cached version of the dataset since allenai/paloma couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'm2d2_s2orc_unsplit' at /home/rd654/.cache/huggingface/datasets/allenai___paloma/m2d2_s2orc_unsplit/0.0.0/65cd6fc59dba021b21db414fa5e8d7765ffbe5e6 (last modified on Sun Dec  1 16:30:46 2024).\n",
      "Map (num_proc=70): 100%|██████████| 167/167 [00:24<00:00,  6.92 examples/s]\n",
      "Using the latest cached version of the dataset since allenai/paloma couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'm2d2_wikipedia_unsplit' at /home/rd654/.cache/huggingface/datasets/allenai___paloma/m2d2_wikipedia_unsplit/0.0.0/65cd6fc59dba021b21db414fa5e8d7765ffbe5e6 (last modified on Sun Dec  1 16:30:47 2024).\n",
      "num_proc must be <= 49. Reducing num_proc to 49 for dataset of size 49.\n",
      "Map (num_proc=49): 100%|██████████| 49/49 [00:07<00:00,  6.30 examples/s]\n",
      "Using the latest cached version of the dataset since allenai/paloma couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'manosphere_meta_sep' at /home/rd654/.cache/huggingface/datasets/allenai___paloma/manosphere_meta_sep/0.0.0/65cd6fc59dba021b21db414fa5e8d7765ffbe5e6 (last modified on Sun Dec  1 16:30:48 2024).\n",
      "Map (num_proc=70): 100%|██████████| 2382/2382 [00:02<00:00, 1129.18 examples/s]\n",
      "Using the latest cached version of the dataset since allenai/paloma couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'mc4' at /home/rd654/.cache/huggingface/datasets/allenai___paloma/mc4/0.0.0/65cd6fc59dba021b21db414fa5e8d7765ffbe5e6 (last modified on Sun Dec  1 16:30:50 2024).\n",
      "Map (num_proc=70): 100%|██████████| 1108/1108 [00:02<00:00, 521.07 examples/s]\n",
      "Using the latest cached version of the dataset since allenai/paloma couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'ptb' at /home/rd654/.cache/huggingface/datasets/allenai___paloma/ptb/0.0.0/65cd6fc59dba021b21db414fa5e8d7765ffbe5e6 (last modified on Sun Dec  1 15:46:25 2024).\n",
      "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  1.28 examples/s]\n",
      "Using the latest cached version of the dataset since allenai/paloma couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'redpajama' at /home/rd654/.cache/huggingface/datasets/allenai___paloma/redpajama/0.0.0/65cd6fc59dba021b21db414fa5e8d7765ffbe5e6 (last modified on Sun Dec  1 16:30:52 2024).\n",
      "Map (num_proc=70): 100%|██████████| 632/632 [00:02<00:00, 309.60 examples/s]\n",
      "Using the latest cached version of the dataset since allenai/paloma couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'twitterAAE_HELM_fixed' at /home/rd654/.cache/huggingface/datasets/allenai___paloma/twitterAAE_HELM_fixed/0.0.0/65cd6fc59dba021b21db414fa5e8d7765ffbe5e6 (last modified on Sun Dec  1 16:30:54 2024).\n",
      "Map (num_proc=70): 100%|██████████| 50000/50000 [00:01<00:00, 27198.56 examples/s]\n",
      "Using the latest cached version of the dataset since allenai/paloma couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext_103' at /home/rd654/.cache/huggingface/datasets/allenai___paloma/wikitext_103/0.0.0/65cd6fc59dba021b21db414fa5e8d7765ffbe5e6 (last modified on Sun Dec  1 16:30:55 2024).\n",
      "num_proc must be <= 59. Reducing num_proc to 59 for dataset of size 59.\n",
      "Map (num_proc=59): 100%|██████████| 59/59 [00:01<00:00, 55.10 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'source', 'input_ids'],\n",
      "    num_rows: 1435\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'source', 'input_ids'],\n",
      "    num_rows: 29016\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# List to store all datasets\n",
    "all_datasets = []\n",
    "subsampled_datasets = []\n",
    "\n",
    "for config_name in config_names:\n",
    "    sub_dataset = load_dataset(\"allenai/paloma\", config_name, split='val', token=os.environ[\"HF_TOKEN\"])\n",
    "    shuffled_sub_dataset = sub_dataset.shuffle(seed=42)  # Set seed for reproducibility\n",
    "\n",
    "    tokenized_sub_dataset = shuffled_sub_dataset.map(\n",
    "        tokenize_and_chunk,\n",
    "        remove_columns=shuffled_sub_dataset.column_names,\n",
    "        batched=True,\n",
    "        batch_size=100,\n",
    "        num_proc=70,\n",
    "        keep_in_memory=True,\n",
    "    )\n",
    "    \n",
    "    # Take up to 100 rows from each dataset\n",
    "    subsampled_dataset = tokenized_sub_dataset.shuffle(seed=42).select(range(min(100, len(tokenized_sub_dataset))))\n",
    "    subsampled_datasets.append(subsampled_dataset)\n",
    "\n",
    "    all_datasets.append(tokenized_sub_dataset)\n",
    "\n",
    "# Combine all datasets into one\n",
    "combined_subsampled_dataset = concatenate_datasets(subsampled_datasets)\n",
    "combined_all_datasets = concatenate_datasets(all_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 11.05ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.65s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 30/30 [00:02<00:00, 10.02ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:11<00:00, 11.59s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/pico-lm/pretokenized-paloma/commit/83e6d221ec4f08ad4c7b1f8fe80a61b0f1e370c8', commit_message='Upload dataset', commit_description='', oid='83e6d221ec4f08ad4c7b1f8fe80a61b0f1e370c8', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_subsampled_dataset.push_to_hub(\"pico-lm/pretokenized-paloma-tinsy\",  token=os.environ[\"HF_TOKEN\"], split=\"val\")\n",
    "combined_all_datasets.push_to_hub(\"pico-lm/pretokenized-paloma\",  token=os.environ[\"HF_TOKEN\"], split=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
